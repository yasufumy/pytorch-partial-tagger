{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ZVoQYJPTBLwn",
        "JoNhrxQvD60i"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip -q install pytorch-partial-tagger"
      ],
      "metadata": {
        "id": "wCgGkIpqYigo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download datasets\n",
        "\n",
        "Here, you would download datasets provided on [teffland/ner-expected-entity-ratio](https://github.com/teffland/ner-expected-entity-ratio/tree/main) below. We use the datasets for the experimental setting Non-Native Speaker Scenario (NNS): Recall=50%, Precision=90% in Effland and Collins. (2021).\n"
      ],
      "metadata": {
        "id": "ZVoQYJPTBLwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -LO https://raw.githubusercontent.com/teffland/ner-expected-entity-ratio/main/data/conll2003/eng/entity.train_r0.5_p0.9.jsonl\n",
        "!curl -LO https://raw.githubusercontent.com/teffland/ner-expected-entity-ratio/main/data/conll2003/eng/entity.dev.jsonl\n",
        "!curl -LO https://raw.githubusercontent.com/teffland/ner-expected-entity-ratio/main/data/conll2003/eng/entity.test.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebARaMdZBPq5",
        "outputId": "9fe0d655-1d76-4569-ae48-2a4fb76fe88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 3918k  100 3918k    0     0  79.7M      0 --:--:-- --:--:-- --:--:-- 79.7M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1165k  100 1165k    0     0  45.5M      0 --:--:-- --:--:-- --:--:-- 45.5M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1114k  100 1114k    0     0  36.2M      0 --:--:-- --:--:-- --:--:-- 36.2M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import all dependencies"
      ],
      "metadata": {
        "id": "JoNhrxQvD60i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "import random\n",
        "from typing import Any\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from partial_tagger.data import CharBasedTags\n",
        "from partial_tagger.recognizer import Recognizer\n",
        "from partial_tagger.training import Trainer\n",
        "from partial_tagger.utils import Metric, create_tag"
      ],
      "metadata": {
        "id": "uXTF2BWeEA-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare datasets\n",
        "\n",
        "You would prepare your datasets. Each item of dataset must have a string and tags. A string represents `text` below. Tags represent a collection of tags, where each tag has a start, a length, and a label, which are defined as `tags` below. A start represents a position in text where a tag starts. A length represents a distance in text between the beginning of a tag and the end of a tag. A label represents what you want to assign to a span of text defined by a start and a length."
      ],
      "metadata": {
        "id": "vDZOzi65Cqhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path: str):\n",
        "    with open(path) as f:\n",
        "        dataset = []\n",
        "\n",
        "        for line in f:\n",
        "            data = json.loads(line.rstrip())\n",
        "\n",
        "            text = \" \".join(data[\"tokens\"])\n",
        "\n",
        "            mapping = {}\n",
        "            now = 0\n",
        "            for i, token in enumerate(data[\"tokens\"]):\n",
        "                mapping[i] = now\n",
        "                now += len(token) + 1  # Add one for a space\n",
        "\n",
        "            tags = tuple(\n",
        "                create_tag(\n",
        "                    mapping[annotation[\"start\"]],\n",
        "                    len(annotation[\"mention\"]),\n",
        "                    annotation[\"type\"],\n",
        "                )\n",
        "                for annotation in data[\"gold_annotations\"]\n",
        "            )\n",
        "\n",
        "            dataset.append((text, CharBasedTags(tags, text)))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_dataset = load_dataset(\"entity.train_r0.5_p0.9.jsonl\")\n",
        "dev_dataset = load_dataset(\"entity.dev.jsonl\")\n",
        "test_dataset = load_dataset(\"entity.test.jsonl\")"
      ],
      "metadata": {
        "id": "1A2K7cMyC1SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train your tagger\n",
        "\n",
        "You would train your tagger by initializing Trainer and passing datasets to it. After training, trainer gives you `Recognizer` object which predicts character-based tags from given texts. Before starting training, we would prepare two utility functions. One is for fixing random state and the other is for displaying training logs."
      ],
      "metadata": {
        "id": "-wbcyARTEHKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_state(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "class JSONAdapter(logging.LoggerAdapter):\n",
        "    def process(self, msg: Any, kwargs: Any) -> Any:\n",
        "        return json.dumps(msg), kwargs\n",
        "\n",
        "\n",
        "def get_logger(log_name: str, log_file: str) -> JSONAdapter:\n",
        "    logger = logging.getLogger(log_name)\n",
        "    logger.propagate = False\n",
        "\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    logger.addHandler(logging.StreamHandler())\n",
        "    logger.addHandler(logging.FileHandler(log_file, mode=\"w\", encoding=\"utf-8\"))\n",
        "\n",
        "    return JSONAdapter(logger)\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "seed = 0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"roberta-base\"\n",
        "batch_size = 15\n",
        "num_epochs = 20\n",
        "learning_rate = 2e-5\n",
        "gradient_clip_value = 5.0\n",
        "padding_index = -1\n",
        "unknown_index = -100\n",
        "train_log_file = \"log.jsonl\"\n",
        "tokenizer_args = {\"padding\": True, \"return_tensors\": \"pt\"}\n",
        "\n",
        "fix_state(seed)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_name,\n",
        "    batch_size,\n",
        "    num_epochs,\n",
        "    learning_rate,\n",
        "    gradient_clip_value,\n",
        "    padding_index,\n",
        "    unknown_index,\n",
        "    tokenizer_args,\n",
        ")\n",
        "\n",
        "recognizer = trainer(\n",
        "    train_dataset,\n",
        "    dev_dataset,\n",
        "    device,\n",
        "    get_logger(f\"{__name__}.train\", train_log_file)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2YEy527GAWp",
        "outputId": "c333336f-239b-4d08-c3e9-41ff0d054905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "{\"epoch\": 1, \"loss\": 93359.27760740928, \"validation_f1_score\": 0.8923462986198244, \"validation_precision\": 0.8870779976717112, \"validation_recall\": 0.8976775496465836}\n",
            "{\"epoch\": 2, \"loss\": 37318.74901078269, \"validation_f1_score\": 0.9184864144024004, \"validation_precision\": 0.9098414795244386, \"validation_recall\": 0.9272972063278357}\n",
            "{\"epoch\": 3, \"loss\": 31896.98245952325, \"validation_f1_score\": 0.9330312185297078, \"validation_precision\": 0.9305323066622029, \"validation_recall\": 0.9355435880175025}\n",
            "{\"epoch\": 4, \"loss\": 28942.03635679814, \"validation_f1_score\": 0.9160982264665757, \"validation_precision\": 0.9284479778776357, \"validation_recall\": 0.9040727027936721}\n",
            "{\"epoch\": 5, \"loss\": 25354.3368652761, \"validation_f1_score\": 0.9203946249362137, \"validation_precision\": 0.9303645116918845, \"validation_recall\": 0.9106361494446314}\n",
            "{\"epoch\": 6, \"loss\": 23358.504559383146, \"validation_f1_score\": 0.9221425558835935, \"validation_precision\": 0.9244038559107052, \"validation_recall\": 0.9198922921575228}\n",
            "{\"epoch\": 7, \"loss\": 21706.889612526516, \"validation_f1_score\": 0.9168023305629338, \"validation_precision\": 0.9338453482283121, \"validation_recall\": 0.9003702457085156}\n",
            "{\"epoch\": 8, \"loss\": 18948.13665033711, \"validation_f1_score\": 0.9116168072956619, \"validation_precision\": 0.9235019858400967, \"validation_recall\": 0.9000336587007741}\n",
            "{\"epoch\": 9, \"loss\": 16663.335592929827, \"validation_f1_score\": 0.8608752449379491, \"validation_precision\": 0.8360291785601015, \"validation_recall\": 0.8872433524065971}\n",
            "{\"epoch\": 10, \"loss\": 14622.866800581949, \"validation_f1_score\": 0.892370922167994, \"validation_precision\": 0.9083144500610075, \"validation_recall\": 0.8769774486704813}\n",
            "{\"epoch\": 11, \"loss\": 12842.631098226615, \"validation_f1_score\": 0.8901854915962728, \"validation_precision\": 0.9223966792997654, \"validation_recall\": 0.8601480982834062}\n",
            "{\"epoch\": 12, \"loss\": 11591.931027319617, \"validation_f1_score\": 0.8721300197781409, \"validation_precision\": 0.8916827852998066, \"validation_recall\": 0.8534163581285762}\n",
            "{\"epoch\": 13, \"loss\": 10065.339028117625, \"validation_f1_score\": 0.8661417322834645, \"validation_precision\": 0.9130759186718895, \"validation_recall\": 0.8237967014473241}\n",
            "{\"epoch\": 14, \"loss\": 9044.909170616666, \"validation_f1_score\": 0.8540088796030296, \"validation_precision\": 0.8845807033363391, \"validation_recall\": 0.8254796364860316}\n",
            "{\"epoch\": 15, \"loss\": 8273.25868594805, \"validation_f1_score\": 0.8503246183540972, \"validation_precision\": 0.8881964809384164, \"validation_recall\": 0.8155503197576573}\n",
            "{\"epoch\": 16, \"loss\": 7932.645848114407, \"validation_f1_score\": 0.8530188347121986, \"validation_precision\": 0.8940959409594096, \"validation_recall\": 0.8155503197576573}\n",
            "{\"epoch\": 17, \"loss\": 7087.735599959677, \"validation_f1_score\": 0.8432698217578366, \"validation_precision\": 0.8815861942353589, \"validation_recall\": 0.8081454055873444}\n",
            "{\"epoch\": 18, \"loss\": 6409.465994215672, \"validation_f1_score\": 0.8333039025251632, \"validation_precision\": 0.8764858841010401, \"validation_recall\": 0.794177044766072}\n",
            "{\"epoch\": 19, \"loss\": 6016.946050590457, \"validation_f1_score\": 0.8344010530934621, \"validation_precision\": 0.8718136805428205, \"validation_recall\": 0.8000673174015484}\n",
            "{\"epoch\": 20, \"loss\": 5734.4184235881985, \"validation_f1_score\": 0.8399680822767975, \"validation_precision\": 0.8875772906127037, \"validation_recall\": 0.7972063278357455}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evalute your tagger\n",
        "\n",
        "You would evaluate the performance of your tagger using Metric as below."
      ],
      "metadata": {
        "id": "KE83TaHIIv1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_scores_file = \"scores.json\"\n",
        "logger = get_logger(f\"{__name__}.evaluate\", test_scores_file)\n",
        "\n",
        "\n",
        "texts, ground_truths = zip(*test_dataset)\n",
        "\n",
        "predictions = recognizer(texts, batch_size, device)\n",
        "\n",
        "metric = Metric()\n",
        "metric(predictions, ground_truths)\n",
        "\n",
        "logger.info({f\"test_{key}\": value for key, value in metric.get_scores().items()})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPYOHjXDI24q",
        "outputId": "681eb8ec-21d6-471f-9e8e-b5e0b3d96a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "{\"test_f1_score\": 0.88945295404814, \"test_precision\": 0.8795222433789164, \"test_recall\": 0.8996104815864022}\n"
          ]
        }
      ]
    }
  ]
}